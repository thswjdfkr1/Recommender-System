{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["6cZSBP00xyzF","jN9jFbfsx222","gukT4euDyNbi","hr5fJm2FyrpQ","k7xNJqmly7jk","-z5U6mKizGHN","dWZA6LudzUuk","r6-aZtoXzbCq","Z7H7fmFIznW1"],"authorship_tag":"ABX9TyOl9okLpqktcSCmvqPrkiC/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SVD (Singular Value Decomposition) - 행렬 분해 기반 추천 모델"],"metadata":{"id":"6cZSBP00xyzF"}},{"cell_type":"code","source":["from surprise import SVD\n","from surprise import Dataset\n","from surprise.model_selection import cross_validate\n","\n","# 내장 데이터셋 (MovieLens 100K)\n","data = Dataset.load_builtin('ml-100k')\n","\n","# SVD 모델 생성\n","model = SVD()\n","\n","# 교차 검증을 통한 성능 평가\n","cross_validate(model, data, cv=5, verbose=True)"],"metadata":{"id":"GdEOnY_XxzQC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# NCF (Neural Collaborative Filtering)\n","\n","행렬 분해(Matrix Factorization, MF)를 딥러닝 기반으로 확장한 추천 모델\n","\n","MF는\n","𝑅=\n","𝑈\n","⋅\n","𝑉\n","𝑇\n","R=U⋅V\n","T형태로 분해하지만,\n","NCF는 이를 신경망을 통해 학습\n","\n","사용자 & 아이템을 임베딩 벡터로 변환 후 신경망을 통해 추천을 수행\n","\n","\n","MF와 MLP를 합친 NeuMF가 가장 강력한 성능을 보임\n"],"metadata":{"id":"jN9jFbfsx222"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n","\n","# 사용자 & 아이템 수 정의 (예제)\n","num_users = 1000\n","num_items = 1000\n","embedding_dim = 32  # 임베딩 차원\n","\n","# 입력 레이어\n","user_input = Input(shape=(1,), name=\"user_input\")\n","item_input = Input(shape=(1,), name=\"item_input\")\n","\n","# 임베딩 레이어\n","user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim)(user_input)\n","item_embedding = Embedding(input_dim=num_items, output_dim=embedding_dim)(item_input)\n","\n","# Flatten (차원 축소)\n","user_vec = Flatten()(user_embedding)\n","item_vec = Flatten()(item_embedding)\n","\n","# MLP 기반 추천 모델\n","concat = Concatenate()([user_vec, item_vec])\n","dense1 = Dense(128, activation='relu')(concat)\n","dense2 = Dense(64, activation='relu')(dense1)\n","dense3 = Dense(32, activation='relu')(dense2)\n","output = Dense(1, activation='sigmoid')(dense3)  # 평점 예측\n","\n","# 모델 정의\n","ncf_model = Model(inputs=[user_input, item_input], outputs=output)\n","ncf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# 모델 구조 출력\n","ncf_model.summary()\n"],"metadata":{"id":"monERPoIyBgO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# AutoEncoder 기반 추천 시스템\n","\n","AutoEncoder를 사용해 사용자-아이템 행렬을 압축(차원 축소)하여 추천\n","\n","기존 MF 모델보다 비선형적인 특징까지 학습 가능\n","\n","희소 행렬을 압축하여 특징을 추출\n","\n","기존 MF보다 비선형적인 패턴을 학습 가능"],"metadata":{"id":"gukT4euDyNbi"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# AutoEncoder 모델 정의\n","class AutoEncoder(nn.Module):\n","    def __init__(self, num_items, hidden_dim=64):\n","        super(AutoEncoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(num_items, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 32),\n","            nn.ReLU()\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(32, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, num_items),\n","            nn.Sigmoid()  # 복원된 사용자-아이템 행렬\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","# 사용자-아이템 행렬 크기 정의\n","num_users, num_items = 1000, 1000\n","model = AutoEncoder(num_items)\n","\n","# 손실 함수 및 옵티마이저 설정\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# 가짜 사용자-아이템 행렬 (랜덤 데이터)\n","input_data = torch.rand((num_users, num_items))\n","\n","# 학습\n","for epoch in range(10):\n","    optimizer.zero_grad()\n","    output = model(input_data)\n","    loss = criterion(output, input_data)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n"],"metadata":{"id":"7LMdUGwMyQQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Graph-based 추천 시스템 (GNN 기반 추천)\n","\n","추천 시스템을 그래프(Graph) 형태로 모델링하여 추천을 수행\n","\n","사용자-아이템 관계를 이웃(Neighbor)으로 간주\n","\n","사용자-아이템 관계를 그래프로 모델링하여 추천을 수행\n","\n","협업 필터링보다 더 복잡한 관계(이웃 사용자, 아이템 간 유사성 등)를 학습 가능"],"metadata":{"id":"hr5fJm2FyrpQ"}},{"cell_type":"code","source":["import dgl\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import dgl.function as fn\n","\n","# 그래프 신경망 모델 (GNN)\n","class GCN(nn.Module):\n","    def __init__(self, in_feats, hidden_feats, out_feats):\n","        super(GCN, self).__init__()\n","        self.layer1 = nn.Linear(in_feats, hidden_feats)\n","        self.layer2 = nn.Linear(hidden_feats, out_feats)\n","\n","    def forward(self, graph, features):\n","        h = torch.relu(self.layer1(features))\n","        graph.ndata['h'] = h\n","        graph.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h'))\n","        h = graph.ndata.pop('h')\n","        h = self.layer2(h)\n","        return h\n","\n","# 사용자-아이템 그래프 생성 (임의 데이터)\n","num_users, num_items = 100, 100\n","edges_src = torch.randint(0, num_users, (500,))\n","edges_dst = torch.randint(0, num_items, (500,)) + num_users\n","\n","graph = dgl.graph((edges_src, edges_dst))\n","features = torch.rand((num_users + num_items, 32))\n","\n","# 모델 정의\n","model = GCN(32, 64, 32)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","loss_fn = nn.MSELoss()\n","\n","# 학습 과정\n","for epoch in range(10):\n","    optimizer.zero_grad()\n","    output = model(graph, features)\n","    loss = loss_fn(output, features)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n"],"metadata":{"id":"um-JPgepyy7O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 시퀀스 데이터 기반 추천 시스템 (Sequential Recommendation)\n","\n","사용자의 과거 행동(클릭, 시청, 구매 등)을 시계열 데이터로 보고, 다음 행동을 예측하는 추천\n","\n","단순한 협업 필터링보다 연속적인 사용자 패턴을 학습할 수 있음\n","\n","Spotify, YouTube 추천: 이전에 들었던 노래/영상 기반으로 다음 곡 추천\n","\n","Netflix, Disney+ 추천: 시청 패턴을 기반으로 추천"],"metadata":{"id":"k7xNJqmly7jk"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# RNN 기반 추천 모델\n","class RNNRecSys(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(RNNRecSys, self).__init__()\n","        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.rnn(x)\n","        out = self.fc(out[:, -1, :])  # 마지막 타임스텝 출력 사용\n","        return out\n","\n","# 입력: 사용자 행동 시퀀스 (예: 최근 본 영화들)\n","model = RNNRecSys(input_dim=32, hidden_dim=64, output_dim=10)\n"],"metadata":{"id":"HCeJHyvEzAmP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Seq2Seq 모델을 추천 시스템에 적용하는 경우\n","\n","**\"입력 시퀀스를 다음 추천 아이템 시퀀스로 변환\"**하는 문제에 적용 가능\n","\n","\"이전에 본 영화 → 다음에 볼 가능성이 높은 영화 시퀀스 추천\"\n","\n","\"유저의 행동 패턴을 시퀀스로 보고, 다음 클릭할 아이템을 예측\""],"metadata":{"id":"-z5U6mKizGHN"}},{"cell_type":"code","source":["class Seq2SeqRecSys(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(Seq2SeqRecSys, self).__init__()\n","        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n","        self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        _, (h, c) = self.encoder(x)\n","        out, _ = self.decoder(x, (h, c))\n","        return self.fc(out[:, -1, :])\n","\n","model = Seq2SeqRecSys(input_dim=32, hidden_dim=64, output_dim=10)\n"],"metadata":{"id":"MFDguusMzGoe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  Attention 기반 추천 시스템 (Attention Mechanism in RecSys)\n","\n","Attention은 사용자의 관심도(Attention Score)를 학습하여 더 적절한 아이템을 추천할 수 있도록 도움\n","\n","TikTok, Instagram Reels 추천: 유저가 특정 스타일의 영상에 더 오래 머물면 해당 스타일에 높은 Attention을 부여\n","\n","E-commerce (아마존, 쿠팡): 사용자가 집중했던 상품의 유사한 상품 추천"],"metadata":{"id":"dWZA6LudzUuk"}},{"cell_type":"code","source":["class AttentionLayer(nn.Module):\n","    def __init__(self, input_dim):\n","        super(AttentionLayer, self).__init__()\n","        self.attn = nn.Linear(input_dim, 1)\n","\n","    def forward(self, x):\n","        attn_weights = torch.softmax(self.attn(x), dim=1)\n","        return torch.sum(attn_weights * x, dim=1)\n","\n","# 사용자의 과거 행동 시퀀스를 Attention을 통해 추천\n","attention_layer = AttentionLayer(input_dim=32)\n"],"metadata":{"id":"BvPAiPSwzZ3H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer가 추천 시스템에서 쓰이는 경우\n","\n","BERT4Rec (Amazon 추천 시스템): BERT 모델을 활용해 순서를 무작위로 바꿔도 추천이 가능하도록 학습\n","\n","SASRec (Self-Attention 기반 추천): LSTM보다 더 강력한 추천 성능\n","\n","GPT 기반 추천 시스템: 대화형 추천 (예: ChatGPT 기반 추천)"],"metadata":{"id":"r6-aZtoXzbCq"}},{"cell_type":"code","source":["class SASRec(nn.Module):\n","    def __init__(self, num_items, embedding_dim):\n","        super(SASRec, self).__init__()\n","        self.embedding = nn.Embedding(num_items, embedding_dim)\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=2),\n","            num_layers=2\n","        )\n","        self.fc = nn.Linear(embedding_dim, num_items)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.transformer(x)\n","        return self.fc(x[:, -1, :])\n","\n","model = SASRec(num_items=1000, embedding_dim=32)\n"],"metadata":{"id":"WrQ3KA0Izdg-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 이미지 기반 추천\n","\n","텍스트+이미지를 결합한 추천 모델이 많이 사용\n","\n","Pinterest, 네이버 쇼핑 → \"유사한 스타일의 패션 아이템 추천\"\n","\n","TikTok, YouTube → \"영상 내용을 분석하여 비슷한 영상 추천\""],"metadata":{"id":"Z7H7fmFIznW1"}},{"cell_type":"code","source":["import torch\n","from transformers import CLIPProcessor, CLIPModel\n","\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","image = torch.rand((3, 224, 224))  # 임의의 이미지 데이터\n","text = [\"Nike 신발\", \"Adidas 신발\", \"Puma 신발\"]\n","inputs = processor(images=image, text=text, return_tensors=\"pt\", padding=True)\n","\n","outputs = model(**inputs)\n","logits_per_image = outputs.logits_per_image\n","recommended_index = logits_per_image.argmax()\n","print(f\"추천 아이템: {text[recommended_index]}\")\n"],"metadata":{"id":"zRV095p4zya1"},"execution_count":null,"outputs":[]}]}